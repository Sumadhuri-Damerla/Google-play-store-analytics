---
title: "Google Play Store Analytics"
author: "Sumadhuri Damerla,Shaoor Jan,Ashjan Khan"
output:
  pdf_document:
    df_print: paged
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: '2'
subtitle: Final Report
---
```{r global_options, include=FALSE}
# global options for figure width and height
knitr::opts_chunk$set(
  fig.width = 10,
  fig.height = 10,
  echo = FALSE,
  warning = FALSE,
  message = FALSE
)

```

\newpage

# Introduction to the dataset  
## Data source
The data source used for this analysis is the *2018 google play store*(https://www.kaggle.com/lava18/google-play-store-apps) collected from Kaggle.

## Description of the dataset
The dataset is a collection of web-scraped data of 10,000 apps from Google Play Store. Google Play Store originally referred as the Android Market, is Google's official store and portal for Android apps, games and other content for Android-powered phone, tablet or Android TV device. As of May 2017, it has over two billion monthly active users, the largest installed base of any operating system, and as of January 2020, the Google Play Store features over 2.9 million apps[13].

The variables of the dataset are as follows:

1)	App (Name) – Name/Title of the application
2)	Category (App)- Category/Domain to which  the app belongs to
3)	Rating (App)- Overall user rating of the app
4)	Reviews (User)- Number of user reviews for the app 
5)	Size (App)- Space or memory that the app takes up 
6)	Installs (App)- Number of user downloads/installs 
7)	Type (Free/Paid)-Apps may be free or paid depending on the developer’s choice
8)	Price (App)-Price of the app if not free
9)	Content Rating -  Age group the app is based off at - Children / Mature 21+ / Adult
10)	Genres (Detailed Category)- An app can belong to multiple genres, For eg, a musical family game will belong to Music, Game, Family genres.
11)	Last Updated (App)- Date when the app was last updated on Play Store 
12)	Current Version (App)- Current version of the app available on Play Store 
13)	Android Version (Support) – minimum version of android it takes to have the app on the device


# Purpose of the project
 + The aim of our project is to find out if we can predict ratings of an app based on different variables and we intend to summarise the different factors that influence the success of an app.These analysis might also help the developer community to build more successful apps by taking accurate data-based decisions, and focusing on those aspects of applications that matters most.

 + Also,since this is the first time we are doing data analysis using R, it is a fun way to learn and  to strengthen the concepts learned during the course by taking a hands-on approach.

# Intended audience of the project
 + We believe there's a diverse set of audience who might be interested in our project. As of Febraury 2020, 73.3% of the mobile operating system market share belongs to Android devices[11]. This large community consists of the general public who use android devices and appstore,the developer community and anyone who wants to understand how the app market works.

 + This project is primarily intended for the growing developer community. It will help them make data backed decisions before launching their application. Besides developers, it is also helpful for tech journalists, Google Play Store users or any other interested party. 


```{r load-libraries}
library(tidyverse)
library(dplyr)
library(stringr)
require(ggExtra) # For marginal graphs
require(GGally) #for correlation plot
library(rpart)
library(moderndive)
library(ggpubr)
theme_set(theme_light())
set.seed(123)
```

```{r custom-color-pallete}
# reference: https://drsimonj.svbtle.com/creating-corporate-colour-palettes-for-ggplot2

custom_colors <- c(
  `red`             = "#d11141",
  `blueish green`   = "#009E73",
  `blue`            = "#0072B2",
  `sky blue`        = "#56B4E9",
  `orange`          = "#E69F00",
  `vermillion`      = "#D55E00",
  `yellow`          = "#009E73",
  `grey`            = "#999999",
  `reddish purple`  = "#CC79A7"
)

custom_cols <- function(...) {
  cols <- c(...)
  
  if (is.null(cols))
    return (custom_colors)
  
  custom_colors[cols]
}

custom_palettes <- list(
  `main`  = custom_cols("blue", "green", "yellow"),
  
  `cool`  = custom_cols("blue", "green"),
  
  `hot`   = custom_cols("yellow", "orange", "red"),
  
  `mixed` = custom_cols("blue", "green", "yellow", "orange", "red"),
  
  `grey`  = custom_cols("light grey", "dark grey")
)

custom_pal <- function(palette = "main",
                       reverse = FALSE,
                       ...) {
  pal <- custom_palettes[[palette]]
  
  if (reverse)
    pal <- rev(pal)
  
  colorRampPalette(pal, ...)
}

scale_color <-
  function(palette = "main",
           discrete = TRUE,
           reverse = FALSE,
           ...) {
    pal <- custom_pal(palette = palette, reverse = reverse)
    
    if (discrete) {
      discrete_scale("colour", paste0("drsimonj_", palette), palette = pal, ...)
    } else {
      scale_color_gradientn(colours = pal(256), ...)
    }
  }

scale_fill <-
  function(palette = "main",
           discrete = TRUE,
           reverse = FALSE,
           ...) {
    pal <- custom_pal(palette = palette, reverse = reverse)
    
    if (discrete) {
      discrete_scale("fill", paste0("drsimonj_", palette), palette = pal, ...)
    } else {
      scale_fill_gradientn(colours = pal(256), ...)
    }
  }
```

```{r loading-data}
clean_play_store <-
  read_csv("data/tidyplaystore.csv") #read_csv returns a dataframe
```

# Exploratory Data Analysis to find factors for success of an app
Generally, the most successful apps have high ratings and high installs.To look at which app makes it to the top, we consider ratings and installs, so we explore these to find any relationship or trends

## Rating column in depth 
### Distribution of rating  
```{r rating-density-plot}
#Density plot of rating
clean_play_store %>%
  ggplot(aes(x = rating)) +
  geom_density(aes(fill = factor(types)), alpha = 0.5) + #types correspond to paid/free
  #adding title, subtitle and labels
  labs(
    title = "Density plot",
    subtitle = "App Rating grouped by types",
    y = "Values",
    x = "Rating",
    caption = "Source: google play store dataset",
    fill = "#Types"
  ) +
  theme(axis.text.x = element_text(vjust = 0.5))
```

**Finding: ** We observed that there are minimal number of apps with low ratings whereas most of the apps have ratings between 3-5.

\newpage

## Correlogram plots  
Firstly, we plot correlograms of rating column versus different columns to find relationships between the variables. Correlograms are useful to understand the relationship between different numerical variables. If the correlogram index is 1, it means that the variables are directly proportional to each other.  
````{r correlogram-plot, out.width="50%", fig.width=3, fig.height=2.5,fig.show='hold',fig.align='center'}

#--code below:correlogram plot of rating vs reviews--
clean_play_store$Reviews <-
  log(clean_play_store$reviews) #transforming the reviews value to logarithmic values

clean_play_store %>%
  select(rating, Reviews) %>%
  ggpairs(mapping = ggplot2::aes(color = "types")) +
  labs(subtitle = "Rating vs Reviews",
       caption = "Source: google play store dataset")

#--code below:correlogram plot of rating vs size--
clean_play_store %>%
  select(rating, size_mb) %>%
  ggpairs(mapping = ggplot2::aes(color = "types")) +
  labs(subtitle = "Rating vs Size (MB)",
       caption = "Source: google play store dataset")

#--code below:correlogram plot of rating vs reviews--
clean_play_store$Installs <-
  log(clean_play_store$installs) #transforming the installs values to logarithmic values

clean_play_store %>%
  select(rating, Installs) %>%
  ggpairs(mapping = ggplot2::aes(color = "types")) +
  labs(subtitle = "Rating vs Installs",
       caption = "Source: google play store dataset")

#--code below:correlogram plot of rating vs types--
clean_play_store$types <-
  as.factor(clean_play_store$types) #changing the types column to factor

clean_play_store %>%
  select(rating, types) %>%
  ggpairs(mapping = ggplot2::aes(color = "types")) +
  labs(subtitle = "Rating vs Types",
       caption = "Source: google play store dataset")
```

**Finding:**  
 * Each correlation index talks about the relationship between plotted columns. If the index is 1, it means they have linear relationship  
 * Each plot on the diagonal refers to the density plot of the respective column  
 * We can observe that there is no significant linear relationship between rating and the plotted numerical variables.  


### Plot of reviews vs app ratings
``` {r review-rating-plot}
#Dot plot to understand ratings vs reviews
ggplot(data = clean_play_store) +
  geom_point(mapping = aes(x = rating, y = reviews, col = types)) +
  scale_color(palette = "hot") + #custom color palette 
  scale_y_continuous(trans = 'log10', labels = scales::comma) + #logarithmic transformation of y-axis
  labs(
    title = "Dot plot",
    subtitle = "Android App Ratings vs Number of Reviews",
    x = "Rating from 1 to 5 stars",
    y = "Number of Reviews",
    caption = "Source: google play store dataset",
    fill = "Types"
  ) +
  theme(legend.title = element_text(hjust = 0.5, face = "bold", size = 10))
```


**Finding:** We can observe that the number of reviews influence the ratings. Generally, as the number of reviews increase, the rating is higher.


### App rating vs category  
```{r rating-category-plot}
#To check the relationship between category and rating, we plotted a box plot with rating on y-axis and category on x-axis.
clean_play_store %>%
  ggplot(aes(x = reorder(category, rating), y = rating)) +
  geom_boxplot(aes(fill = category)) +
  scale_fill(palette = "main") +
  labs(
    title = "Boxplot",
    subtitle = "Android App Ratings by category",
    x = "Category",
    y = "Rating",
    caption = "Source: google play store dataset"
  ) +
  theme(axis.text.x = element_text(angle = 90),
        legend.position = "None") +
  coord_flip() #coordinate flip to view the graph better
```

**Finding:** We observed that some categories like TOOLS, FAMILY, FINANCE and LIFESTYLE and a great majority of applications fall below first quartile. Thus, even though the median rating is high, the deviation from median is significant.

\newpage

### Distribution of rating for 8 categories with the largest numbers of apps   
We look at the distribution of rating across different categories. We chose 8 categories with the largest number of applications.  

```{r avg-rating-distributionplot}
# Distribution of rating for 8 categories with the largest numbers of apps
clean_play_store %>%
  #adding a filter to chose top 8 categories
  filter(
    category == c(
      "FAMILY",
      "GAME",
      "TOOLS",
      "MEDICAL",
      "BUSINESS",
      "PRODUCTIVITY",
      "FINANCE",
      "COMMUNICATION",
      "SPORTS"
    )
  ) %>%
  #facet plot of rating
  ggplot(aes(x = rating)) +
  geom_bar(fill = custom_cols("blueish green"), na.rm = TRUE) +
  facet_wrap(~ category,
             ncol = 4,
             nrow = 4,
             shrink = TRUE) +
  labs(
    title = "Facet plot",
    subtitle = "Distribution of rating for 8 categories with the highest numbers of apps",
    x = "Rating",
    y = "Count",
    caption = "Source: google play store dataset"
  )
```

**Finding:** The distribution of rating varies significantly across each categories.  

\newpage

### Average rating per category 
In the previous graph, we observed that the distribution of rating varies significantly with different categories. This plot is to find the average rating per category.

```{r avgrating-categoryplot}
# Average rating per category
clean_play_store %>%
  #select category and rating columns
  select(c("category", "rating")) %>%
  #grouping the dataset by category
  group_by(category) %>%
  summarise(rating = mean(rating, na.rm = TRUE)) %>%
  #bar plot of the average rating and category
  ggplot(aes(x = reorder(category, -rating) , rating)) +
  geom_col(width = 0.5, fill = custom_cols("sky blue")) +
  #title
  ggtitle("Average Rating Per Category") +
  #Adding labels
  labs(
    title = "Bar Chart",
    subtitle = "Average Rating Per Category",
    y = "Rating",
    x = "Category",
    caption = "Source: google play store dataset"
  ) +
  #adding the theme
  theme(axis.text.x = element_text(angle = 90),
        legend.position = "none")
```

**Finding:** We observed that the average rating per category is not very different for each category. The "EVENTS" category still has the highest average rating, and "DATING" category has the least average rating. 

\newpage

### Size and rating  
The below plot is to verify if application sizes influence the rating. A marginal plot is a scatterplot that has histograms, boxplots, or dotplots in the margins of the x- and y-axes.

```{r size-rating-plot}
#Marginal plot of size vs rating
plot <- clean_play_store %>%
  ggplot(aes(x = size_mb, y = rating, na.rm = TRUE)) + #na.rm removes NA values
  geom_point(color = custom_cols("blueish green"), na.rm = TRUE) +
  labs(
    title = "Marginal Plot",
    subtitle = "Size Vs Rating",
    y = "Rating",
    x = "Size (MB)",
    caption = "Source: google play store dataset"
  )

#Marginal plot
ggMarginal(
  plot,
  type = "histogram",
  fill = custom_cols("blueish green"),
  alpha = 0.6
)
```

**Finding:** We observed that majority of applications with sizes under 25 MB have a good rating (above 4).


## Exploring installs column
### Number of installs per category
```{r installs-percategory}

#Bar plot of installs vs category
clean_play_store %>%
  count(category, installs, types) %>%
  #group by category and types column
  group_by(category, types) %>%
  summarize(totalInstalls = sum(as.numeric(installs))) %>% #calculating the total number of installs
  head(33) %>%
  ggplot(aes(
    x = reorder(category, totalInstalls),
    y = log(totalInstalls), #log transforming because the values range till a billion
    fill = types
  )) +
  #setting the bar width to 0.5
  geom_bar(width = 0.5, stat = "identity") +
  #formating the label
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = "Bar Chart",
    subtitle = "Number of Installs per Category",
    x = "Category",
    y = "Number of Installs(log)",
    caption = "Source: google play store dataset"
  ) +
  scale_fill(palette = "main") +
  theme(axis.text.x = element_text(angle = 0)) +
  #flipping the coordinate
  coord_flip()
```

**Finding:** The graph shows the log of number of installs (the values of installs varied from 0 to 1 billion) vs CATEGORY. FAMILY and GAME have the highest number of installs. EVENTS, HOUSE_AND_HOME, COMICS, LIBRARIES_AND_DESIGN and BEAUTY have the least number of installs.

### Top 10 installed categories  

```{r top10-installed-categories}
#Top 10 categories with most number of installs
clean_play_store %>%
  count(category, installs) %>%
  group_by(category) %>%
  summarize(totalInstalls = sum(as.numeric(installs))) %>%
  #arranging in desc order of totalinstalls to get the top installed categories
  arrange(-totalInstalls) %>%
  #taking the top 10
  head(10) %>%
  #Bar plot of category vs total installs
  ggplot(aes(
    x = reorder(category, -totalInstalls),
    y = totalInstalls/1000000,
    fill = custom_cols("red")
  )) +
  geom_bar(width = 0.5,
           fill = custom_cols("blueish green"),
           stat = "identity") +
  labs(
    title = "Bar Chart",
    subtitle = "Top 10 Installed Categories",
    x = "Category",
    y = "Number of Installs(Million)",
    caption = "Source: google play store dataset"
  ) +
  #changing the format of labels of installs
  scale_y_continuous(labels = scales::comma) +
  theme(axis.text.x = element_text(angle = 360, vjust = 0.5),
        legend.position = "None") +
  #flip the coordinate
  coord_flip()
```

**Finding:** The apps under Communicationcategory have more than 1500M number of installs. 

### 10 least installed categories
```{r bottom10-installed-categories}
clean_play_store %>%
  count(category, installs) %>%
  group_by(category) %>%
  #computing the total number of installs
  summarize(totalInstalls = sum(as.numeric(installs))) %>%
  #arranging the dataset in descending order of totalInstalls
  arrange(-totalInstalls) %>%
  #taking the last 10 records
  tail(10) %>%
  ggplot(aes(x = reorder(category, totalInstalls), y = totalInstalls/1000000)) +
  geom_bar(width = 0.5,
           fill = custom_cols("vermillion"),
           stat = "identity") +
  labs(
    title = "Bar Chart",
    subtitle = "10 Least Installed Categories",
    x = "Category",
    y = "Number of Installs(Million)",
    caption = "Source: google play store dataset"
  ) +
  scale_y_continuous(labels = scales::comma) +
  theme(axis.text.x = element_text(angle = 0, vjust = 0.5),
        legend.position = "None") +
  #flipping the coordinate
  coord_flip()
```

**Finding:** We observed that applications under "Events" category have the least number of installed applications followed by medical and education categories. 

### Top 10 paid Categories
```{r top-paid-categories-plot}
#Top 10 categories with the highest number of installs for paid applications.
clean_play_store %>%
  filter(types == "Paid") %>% #filtering for paid apps
  group_by(category) %>%
  summarize(totalInstalls = sum(installs)) %>%
  arrange(desc(totalInstalls)) %>% #desc order of totalinstalls
  head(10) %>% #taking the first 10 records
  ggplot(aes(x = reorder(category, -totalInstalls) , y = totalInstalls/1000000)) +
  geom_bar(fill = custom_cols("blueish green"),
           stat = "identity",
           width = 0.5) +
  labs(
    title = "Bar Chart",
    subtitle = "Top 10 Paid Categories",
    x = "Category",
    y = "Number of Installs(Million)",
    caption = "Source: google play store dataset"
  ) +
  scale_y_continuous(labels = scales::comma) +
  theme(axis.text.x = element_text(angle = 65, vjust = 0.5),
        legend.position = "None") 
```

**Finding:** Applications with family and game category with 20M installs have the highest number of installs.

### Size Vs. number of installs    
Size is an important characteristic of an application. Large applications might reduce the number of installs, as it reduces to targeted audience. We can test this by plotting size against installs. 
```{r size-installs-plot}

plot <- clean_play_store %>%
  filter(!is.na(size_mb)) %>% #removing NA values
  ggplot(aes(
    x = size_mb,
    y = log(installs + 0.005),
    na.rm = TRUE
  )) +
  geom_point(color = custom_cols("vermillion"), na.rm = TRUE) +
  labs(
    title = "Marginal Plot",
    subtitle = "Size Vs Number of Installs",
    y = "No. of Installs (Log)",
    x = "Size (MB)",
    caption = "Source: google play store dataset"
  )
#marginal plot
ggMarginal(plot,
           type = "histogram",
           fill = custom_cols("vermillion"),
           alpha = 0.6)
```

**Finding:** Optimally sized applications with sizes between 5MB and 30MB have the greatest number of installs. 


### Number of installs based on support by minimum android version
```{r android_ver,warning=FALSE,message=FALSE}
#Bar plot of min android version vs types
clean_play_store %>%
  ggplot(aes(x = min_android_ver, fill = types)) +
  geom_bar(width = .5,
           alpha = 0.7,
           na.rm = TRUE) +
  labs(
    title = "Bar Chart",
    subtitle = "Installs Based on Min. Android Version",
    x = "Min. Android Version",
    y = "Count",
    caption = "Source: google play store dataset",
    fill = "Types"
  ) +
  theme(
    legend.title = element_text(hjust = 0.5, face = "bold", size = 10),
    axis.text.x = element_text(angle = 90)
  )
```

**Finding:** 4.1 android version has the maximum number of installs. This implies that most google play store users use a minimum of 4.1 android version.

\newpage

# Summary of EDA
 + To get most success from an app, it has to have maximum number of reviews and maximum number of ratings. These are the other trends we found:  
   1) Apps should atleast support 4.1 android version or more to succeed.This is expected because the majority of users have smart phones with constant android updates.  
   2) They have high chances of success if the genre is family, game, communication or productivity apps whereas food_and_drink, auto_vehicles categories have very low probability to succeed.  
   3) The apps which are be free have huge success rates however there are a few exceptions to this.  
   4) Apps should be optimally sized between 5MB and 30MB.  



# Future work (models)  
## What are we trying to achieve (thesis/hypothesis)
The dataset contains information about ten thousand apps dated till the year 2018. We plan on using Linear Regression Model, Recursive Partitioning Model and Random Forest Model to perform predictive analysis. We want to answer the following hypothesis:  

1) Find similarities in apps that make it to the top of Play Store. Factors contributing to the success of applications.
2) Can we predict rating of apps based on other parameters such as number of reviews or the size of an app?

To answer these questions, we will be exploring all the variables of this dataset to find if there's any relationship between rating and other variables. We intend to find out which of these variables will play the most important role in predicting rating.
 

## Why is this important/interesting?
Before we started this project, we took part in a competition called game-jam [12] where we built a game,we had an idea of launching it on Google Play Store.We then thought it would be interesting to see currents trends in the market and to do a detailed analysis to get more insights to the following questions:  
1.Factors that influence the success of an app,    
2.Which categories are highly installed    
3.What are the most famous applications and do they have any trends in common like number of installs, number of reviews, size or android-version? and etc  

This analysis inturn will aid the developer community to build successful apps targeting a specific audience.


## How are we going to test the hypothesis?
Since the dataset is dated till 2018.  We are thinking to test it by comparing the predictions of our model with the 2019 dataset if possible or we will test our model with dummy application 


## Any challenges that we might encounter
We cannot ensure complete accuracy of the model because there is a lot of useful information missing that could have given us more insight into the Google Play Store market e.g. Demographic data could have offered insights into the rating and number of installs of apps, with respect to different regions,  different cultures and different trends popular to specific age groups. Also, it would have been interesting to see how different global trends affect the usage of the app, for instance, the current pandemic "covid-19" has called for quarantine across the globe and many people, markets and other companies are relying on smart phones and virtual connections, this will heavily increase the use of many applications, thus deviating from the general trend. 

# Model
 + The goal is to predict the rating of an application based on different parameters. Our EDA confirms that variables like number of installs, number of reviews, category of the app and the type of app (paid/free) can affect the rating of an app.  

 + In this section,firstly, we tried to fit our dataset for both explanatory and predictive models. We used the cross-validation technique to select the best fitting model for our dataset and lastly, we test the models by predicting ratings for a dummy app.

## Correlation using Pearson method
 + The bivariate Pearson correlation indicates whether a statistically significant linear relationship exists between two continuous variables or it indicates the strength of a linear relationship (i.e., how close the relationship is to being a perfectly straight line)

 + Before creating a model, we calculated the correlation between different variables to help us select appropriate exploratory variables.
```{r correlation_rating_installs}

# Finding Correlation between rating and installs column
cor_rat_ins <- cor(
  clean_play_store$installs,
  clean_play_store$rating,
  use = "complete.obs" ,
  method = "pearson"
)
names (cor_rat_ins) <- "Rating and Installs"


# Finding Correlation between rating and log10(installs)^2
cor_rat_ins_transformed <- cor(
  log10(clean_play_store$installs) ^ 2,
  clean_play_store$rating ,
  use = "complete.obs",
  method = "pearson"
)

names(cor_rat_ins_transformed) <- "Rating and Installs(transformed)"

# print out both values
knitr::knit_print(cor_rat_ins)
knitr::knit_print(cor_rat_ins_transformed)

```
+ **Correlation coefficient between rating and installs**: The correlation coefficient is 0.0402. This indicates a slight positive relation between these two variables. Using square of log of installs, increased the correlation coefficient to 0.1138.

 
```{r correlation_rating_reviews}

# Finding Correlation between rating and reviews
cor_rat_rev <- cor(
  clean_play_store$reviews,
  clean_play_store$rating,
  use = "complete.obs",
  method = "pearson"
)

names(cor_rat_rev) <- "Rating and Reviews"

# Finding Correlation between rating and log10(reviews)^2
cor_rat_rev_transformed <- cor(
  log10(clean_play_store$reviews) ^ 2,
  clean_play_store$rating ,
  use = "complete.obs",
  method = "pearson"
)

names(cor_rat_rev_transformed) <- "Rating and Reviews(transformed)"

# print out both values
knitr::knit_print(cor_rat_rev)
knitr::knit_print(cor_rat_rev_transformed)

```
 + **Correlation coefficient between rating and reviews**: The correlation coefficient is 0.05515, indicating a slight positive relationship between the two variables. Using square of log of installs, increased the correlation coefficient to 0.20308.
 
\newpage
 
## Explanatory/Descriptive modeling
* The following models are used: 
 + Linear Regression Model
 + Local Regression Model
 + Recursive Partitioning Model

```{r descriptive_modeling_1, warning=FALSE, message=FALSE,fig.show='hold',fig.align='center'}
# Turn off scientific notation
options(scipen = 999)

# Function for creating jitter plot
jitterPlot <- function(x_var) {
  if (x_var == "installs") {
    x_axis_label <- "Installs(log10^()2 transformed)"
    var_selected <- log10(clean_play_store$installs) ^ 2
    
  } else if (x_var == "size_mb") {
    x_axis_label <- "Size(log10 transformed)"
    var_selected <- log10(clean_play_store$size_mb)
    
  } else if (x_var == "reviews") {
    x_axis_label <- "Reviews(log10^()2 transformed)"
    var_selected <- log10(clean_play_store$reviews) ^ 2
  }
  
  #Jitter plot of selected variable vs rating for rpart, loess and lm models
  clean_play_store %>%
    ggplot(aes_string(x = var_selected, y = "rating")) +
    scale_x_log10() + #log 10 transformation
    geom_jitter(na.rm = TRUE,
                color =  custom_cols("blueish green"),
                alpha = .4) +
    geom_smooth(
      method = "rpart",
      se = FALSE,
      aes(color = "orange"),
      na.rm = TRUE
    ) +
    geom_smooth(method = "loess",
                se = FALSE,
                aes(color = "red"),
                na.rm = TRUE) +
    geom_smooth(method = "lm",
                se = FALSE,
                aes(color = "blue"),
                na.rm = TRUE) +
    labs(
      title = "Jitter Plot",
      subtitle = paste (x_axis_label, "vs Rating", collapse = " "),
      x = x_axis_label,
      y = "Rating",
      caption = "Source: google play store dataset"
    ) +
    theme(legend.title = element_text(
      hjust = 0.5,
      face = "bold",
      size = 10
    )) +
    scale_color_manual(
      name = 'Models',
      values = c(
        "orange" = "orange",
        "red" = "red",
        "blue" = "blue",
        "vermillion"
      ),
      labels = c(
        "Linear Regression Model",
        "Recursive Partitioning Model",
        "Local Regression Model"
      )
    )
}

jitterPlot("installs") # Creating a jitter plot: installs vs rating
jitterPlot("size_mb") # Creating a jitter plot: size_mb vs rating
jitterPlot("reviews") # Creating a jitter plot: reviews vs rating
```

The graphs above show the fitted Linear Regression model, Local Regression model and Recursive Partitioning model for predicting rating using number of installs, size of the app and number of reviews variables. 


## Explanatory/Descriptive modeling (cont.) 
Continuing with our explanatory modeling, we will fit a model for rating vs installs given taking into consideration whether the app is Free or Paid.  We have also calculated sum of square residuals, R-Square value and Root Mean Square Error for the model. 
```{r descriptive_modeling_2}

# Fitting a lm model
model_rat_instl <-
  lm(rating ~ installs + types + category + reviews, data = clean_play_store)


# Getting Regression points table
points_table <- get_regression_points(model_rat_instl)

points_table

# Fitting a model taking into consideration the type of app(Free or Paid)
points_table %>%
  ggplot(aes(x = log10(installs + .0005), y = rating)) +
  geom_jitter(na.rm = TRUE,
              color =  custom_cols("sky blue"),
              alpha = .4) +
  geom_smooth(aes(color = types), method = "lm", se = FALSE) +
  labs(
    title = "Jitter Plot",
    subtitle = "Log10(Installs) vs Rating",
    x = "Log10(Installs)",
    y = "Rating",
    color = "Types",
    caption = "Source: google play store dataset"
  ) +
  theme(legend.title = element_text(hjust = 0.5, face = "bold", size = 10))

# Calculating Sum of Square residuals
points_table %>%
  summarise(square = sum(residual ^ 2))

# Calculating R-Squared to values
points_table %>%
  summarise(r_squared = 1 - var(residual) / var(rating))

# Calculating RMSE
points_table %>%
  summarise(RMSE = sqrt(mean(residual ^ 2)))

```
**Observations/Findings**: We observe that the rating of paid apps is higher than the free apps indicating better quality of paid apps. On the other hand, free apps have a higher number of installs, thus covering a wide range of audience.



## Cross Validation of models
This technique helps us identify the best fit model for our dataset i.e. model with the least root mean square error
```{r cross_validation, warning=FALSE,message=FALSE}

# Spliting our dataset into two dattasets i.e. train(75%) and test(25%)
smp_size <- floor(0.75 * nrow(clean_play_store))

train_ind <-
  sample(seq_len(nrow(clean_play_store)), size = smp_size)
train <- clean_play_store[train_ind,]
test <- clean_play_store[-train_ind,]

# Fitting our models, Linear Regression model and Recursive Partitioning model
model_lm_rat_instl <-
  lm(rating ~ installs + types + category + reviews, data = train)
model_rpart_rat_inst <-
  rpart(rating ~ installs + types + category + reviews , data = train)

# Predicting rating for test dataset
predicted_rating_lm <- predict(model_lm_rat_instl, newdata = test)
predicted_rating_rpart <-
  predict(model_rpart_rat_inst, newdata = test)

# Calculating error
error_lm <- test$rating - predicted_rating_lm
error_rpart <- test$rating - predicted_rating_rpart

# Calculating Root Mean Square Error
RMSE_lm <- sqrt(mean(error_lm ^ 2, na.rm = TRUE))
RMSE_rpart <- sqrt(mean(error_rpart ^ 2, na.rm = TRUE))

RMSE_lm
RMSE_rpart

```
  + We sliced the dataset into training and test datasets. Training dataset contain 6,951 observations (75% of total) and test data set contains 2,198 observations (25% of total). 
  + We trained both Linear Regression model (lm) and Recursive Partitioning model (rpart) with the training dataset.
  + After training our model, we predicted the rating for test dataset. In order to find which model fits better, we calculated the root mean square error (RMSE) for both models. RMSE values for Linear Regression model and Recursive Partitioning model are 0.5277467 and 0.5183104 respectively. We can see that RMSE value for Recursive Partitioning model is less than RMSE value for Linear Regression model implying that Recursive Partitioning model can make a better prediction than Linear Regression model. 
  + Along with performing a comparison between Linear Regression model and Recursive Partitioning model, we experimented with different explanatory variables for each model. We got the least RMSE by using installs, types, categories and reviews as explanatory variables.

## Making Predictions 
We can predict rating of existing applications by entering different parameters like installs, types, category and reviews 
```{r predictive_modeling, warning=FALSE,message=FALSE}

#code below:create a function to predict rating when the inputs are given
predict_rating <- function(installs,type,category,reviews){
  #construct a predict dataset based on the inputs
  predict_app <-
  data.frame(
    "installs" = installs,
    "types" = type,
    "category" = category,
    "reviews" = reviews
  )
#Use recursive partitioning model to predict the ratings
predicted_rating_rpart <-
  predict(model_rpart_rat_inst, newdata = predict_app)
#return the predicted rating value
 predicted_rating_rpart
}

#App1: Nike Training Club - Workouts & Fitness Guidance (data from [6])
#Actual rating: 4.2
predict_rating(
    installs = 100000000,
    type = "Free",
    category = "HEALTH_AND_FITNESS",
    reviews = 272000
  )

#App2: Wedding Planner & Organizer, Guest Checklists (data from [6])
#Actual rating:4.5
predict_rating(
    installs = 10000,
    type = "Free",
    category = "EVENTS",
    reviews = 220
  )

#App3: Mario Kart Tour (data from [6])
#Actual rating:4
predict_rating(
    installs = 5000000,
    type = "Free",
    category = "GAME",
    reviews = 1000000
  )

```
 + **1st application**: First dummy application of type Free with 1 billion installs and 100000 reviews, belonging to TOOLS category has a predicted rating of 4.2853. This to be a reasonable prediction A Free app with 1 billion installs and 100,000 reviews must be a popular app and is expected to have high rating. But the fact that it belongs to "Tools" category, which has the third least average rating, its rating is not exceptionally high. Its rating is much better than average (4.17) but is not among the highest ratings.
 
 + **2nd application**: Second dummy application is of type Free, with 1000 installs and 100 reviews, belonging to EVENTS category, has a predicted rating of 4.1313. This seems to be a reasonable prediction. A free app is expected to have higher installs, but as this app does not have high number of installs, indicating that the app is not the best in its segment or it has limited relevance or utility, thus we can expect low rating. But, as it belongs to EVENTS category, which has the highest average rating among all categories, its rating is not among the worst. It has a below average rating but is still not among the wort rated apps. 
 
 
 ## **Load data and libraries**

```{r warning = FALSE, message = FALSE}


library(tidyverse)
library(dplyr)
library(ggplot2)
library(magrittr)
library(caret)
library(randomForest)
google <- clean_play_store
```

*check the data structure* 
```{r check-data}
str(google)
```

*Lets check the number of _missing values_ per column*

```{r check-na}
sapply(google, function(x) sum(is.na(x)))
```

The **Rating** and **max_android_ver**column has a large number of missing values - too many to be imputed with confidence.  
Running na.omit() leaves us with plenty of data for modelling, so let's make life a bit easier:

```{r omit-na}
google %<>% na.omit() 
```

## **Preparing data for machine learning**

### Check that "Reviews" are related to "Installations"


Showing that **Reviews** are a good indicator of **Installations**  
```{r reviews-installations ,echo = F}
google$Reviews <- as.numeric(google$reviews)

reviews <- google %>% 
            group_by(installs) %>% 
            summarise(Avg_reviews = mean(reviews)) %>%
            arrange(-Avg_reviews)
            
knitr::kable(reviews)
```

### Data transformations

The predictor **Rating** and the outcome variable **Reviews** are 
highly skewed: 

```{r echo = F}
hist(google$rating, col = "orange", main = "Rating")
hist(google$reviews, col = "lightblue", main = "Reviews")
```



**Finding:** The raw rating and reviews are highly skewed.

+We will transform the data to remove the skewedness. Since reviews are rightly skewed, a log transformation can be applied.As for ratings, we will be applying the powers of 4 to remove the left skewedness. 
```{r transformation-reviews_ratings}
google$reviews <- log(google$reviews)
google$rating <- google$rating^4.1
hist(google$rating, col = "orange", main  = "Rating transformed")
hist(google$reviews, col = "lightblue", main = "Reviews transformed")

```




**Finding**-Now that the ratings and reviews are transformed for our model, Lets select category, types, reviews and ratings as the primary features for our model building.Since category and types are categorical variables we will dummify them(convert them to numeric data) to create a uniform analysis of the model.

## Dummify the data & remove zero variance predictors
```{r dummify-remove-zero-variance}
#Selecting primary features
googleML <- google %>% select(category, types, reviews, rating)
#converting categorical data to numeric data
googleML$category <- as.factor(googleML$category)
googleML$types <- as.factor(googleML$types)
#dummifying variables
dummy <- dummyVars(" ~ .", googleML)
googleML <- data.frame(predict(dummy, googleML))
#Removing zero variance variables(types.paid)
zero_var <- nearZeroVar(googleML)
googleML <- googleML[, -zero_var] %>% select(-c(types.Paid))
```


### Prepare the train and test sets

```{r train-test}
set.seed(100)
rows <- createDataPartition(googleML$rating, list = F, p = 0.7)
gTrain <- googleML[rows,]
gTest <- googleML[-rows,]
```

## **Random forest model analysis**
Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction.
The fundamental concept behind random forest is a simple but powerful one — the wisdom of crowds. In data science speak, the reason that the random forest model works so well is:
A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.
A Random Forest is an ensemble technique capable of performing both regression and classification tasks with the use of multiple decision trees and a technique called Bootstrap Aggregation, commonly known as bagging. What is bagging you may ask? Bagging, in the Random Forest method, involves training each decision tree on a different data sample where sampling is done with replacement.




**Model Analysis**

```{r model-analysis ,message = F, warning = F}
#creating a random forest object
forestGoogle <- train(reviews ~ ., data = gTrain, method = "ranger", tuneLength = 4, trControl = trainControl(method = "cv", number = 5, verboseIter = F))
# an object for computing error plot and compute error figures and variable importance
error <- randomForest(reviews ~ ., data = gTrain, method = "ranger", tuneLength = 4, trControl = trainControl(method = "cv", number = 5, verboseIter = F), Importance = TRUE)
```

```{r variable-importance}
##Running variable importance plot to compute what variables are influencing the model the most.
imp <- varImpPlot(error, sort = TRUE)
# let's save the varImp object
# this part just creates the data.frame for the plot part
imp <- as.data.frame(imp)
imp$varnames <- rownames(imp) # row names to column
imp$var_categ <- rep(1:6)
rownames(imp) <- NULL  

#plotting variable importance
ggplot(imp, aes(x=reorder(varnames, IncNodePurity), weight=IncNodePurity, fill=as.factor(var_categ))) + 
  geom_bar() +
  labs(title = "variable importance")+
  scale_fill_discrete(name="Variable Group") +
  ylab("IncNodePurity") +
  xlab("Variable Name")+
  coord_flip()

```







**Finding:** The model considers *rating* as the most important variable in the model building followed by *category.NEWS_AND_MAGAZINES* that has the least influence.The value *IncNodePurity* suggests the  measure of variable importance based on the Gini impurity index used for the calculating the splits in trees.
The higher the value of mean decrease accuracy or mean decrease gini score, the higher the importance of the variable to our model.

**Error analysis**
A well-fitting regression model results in predicted values close to the observed data values. The mean model, which uses the mean for every predicted value, generally would be used if there were no informative predictor variables. The fit of a proposed regression model should therefore be better than the fit of the mean model.

The RMSE is the square root of the variance of the residuals. It indicates the absolute fit of the model to the data–how close the observed data points are to the model’s predicted values. Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit. As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable. Lower values of RMSE indicate better fit. RMSE is a good measure of how accurately the model predicts the response, and it is the most important criterion for fit if the main purpose of the model is prediction.

```{r error-result}
plot(error)
error_figure <- getTrainPerf(forestGoogle)
knitr::kable(error_figure)
```


**Finding:**This error plot provides and unbiased estimate of the error rate.Each tree uses a different bootstrap sample (~1/3 of samples) for testing.
We observe that the error is high among the top 50 sample data trees.
Also the trainRMSE(Root Mean Square Error) is 3.065 which is a very high figure for the data we have which implies the model did not do very well with the trained data.


```{r model-process-sketch}
 forestGoogle
 plot(forestGoogle)
```

**Observation**- This plot shows the RMSE(Root Mean Square Error) with the randomly selected predictors that the model took.We also see here that the RMSE falls and rises with variance and trees generated 
Since the RMSE reaches a high value , the model's efficiency to work on the accuracy of the prediction is low.A high RMSE implies  a model that tests well in sample, but has little predictive value when tested out of sample.


```{r compare-actual-predicted-reviews}
#predict with test data
predForest <- predict(forestGoogle, gTest)
#creating a dataframe for plotting 
forest <- data.frame(pred = predForest, actual = gTest$reviews)
#plotting actual vs. predicted results
ggplot(forest, aes(pred, actual)) +
stat_binhex(colour = "black") +
theme_bw() +
xlim(0, 20) +
ylim(0, 20) +
ggtitle("App reviews - Predicted vs. Actual") +
theme(aspect.ratio=1) +
scale_fill_gradient(limits=c(0, 500), low="red", high="blue")

```

**Observation**- The actual and predicted values of the app reviews match at very low counts as generated from the model's graph.

# **Conclusion**
The  Random forest model used here hasn't done a great job at predicitng app popularity and appears to underestimate the overall popularity of the test set. The results are somewhat encouraging - some of the variability in Reviews has been accounted with relatively simple models. App rating is therefore likely to be an important factor in determining app popularity.



# References
[1] [Dataset](https://www.kaggle.com/lava18/google-play-store-apps);  
[2] [R markdown](https://bookdown.org/yihui/rmarkdown/html-document.html#tabbed-sections) ;  
[3] [Stackoverflow](https://stackoverflow.com/questions/3993301/how-to-format-number-values-for-ggplot2-legend/15007117) ;  
[4] [GGally](https://ggobi.github.io/ggally/rd.html#ggpairs);  
[5] [Custom color pallete](https://drsimonj.svbtle.com/creating-corporate-colour-palettes-for-ggplot2);   
[6] [App details] (https://play.google.com/store/apps);
[7] [Colors](https://hbr.org/2014/04/the-right-colors-make-data-easier-to-read);  
[8] [TidyVerse](https://www.tidyverse.org/);  
[9] [ggplot2](https://ggplot2.tidyverse.org/reference/);  
[10] [Google playstore Kernel by Danilodiogo](https://www.kaggle.com/danilodiogo/google-play-store-eda-plotting-with-highcharts/code#eda);  
[11] [market share](https://gs.statcounter.com/os-market-share/mobile/worldwide) ;  
[12] [Global game jam](https://globalgamejam.org/) ;  
[13] [Play Store Statistics](https://www.statista.com/statistics/266210/number-of-available-applications-in-the-google-play-store/) ; 
